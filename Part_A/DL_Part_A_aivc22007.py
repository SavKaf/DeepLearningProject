# -*- coding: utf-8 -*-
"""DL_Part_A_aivc22007.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gkwjjT2L-mlUwG3INJ3AULzC2toKyx7s

# Deep Learning
Project 2023, Part A

Student: Kaftantzis Savvas , aivc22007
"""

# Commented out IPython magic to ensure Python compatibility.
#Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
# %matplotlib inline

# Import our dataset and check the head of it
from google.colab import drive
drive.mount('/content/drive')
# Import our dataset and check the head of it
df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Dataset2Use_PartA.xlsx')
df.head()

# Drop the last column (year)
df.drop(df.columns[-1], axis=1, inplace=True)

# Rename columns
df.rename(columns={'365* ( Β.Υ / Κοστ.Πωλ )': 'A',
                   'Λειτ.Αποτ/Συν.Ενεργ. (ROA)': 'B',
                   'ΧΡΗΜ.ΔΑΠΑΝΕΣ / ΠΩΛΗΣΕΙΣ': 'C',
                  ' ΠΡΑΓΜΑΤΙΚΗ ΡΕΥΣΤΟΤΗΤΑ :  (ΚΕ-ΑΠΟΘΕΜΑΤΑ) / Β.Υ': 'D',
                  '(ΑΠΑΙΤ.*365) / ΠΩΛ.': 'E',
                  'Συν.Υποχρ/Συν.Ενεργ': 'F',
                  'Διάρκεια Παραμονής Αποθεμάτων': 'G',
                  'Λογαριθμος Προσωπικού': 'H',
                  'ΕΝΔΕΙΞΗ ΕΞΑΓΩΓΩΝ': 'I',
                  'ΕΝΔΕΙΞΗ ΕΙΣΑΓΩΓΩΝ': 'J',
                  'ΕΝΔΕΙΞΗ ΑΝΤΙΠΡΟΣΩΠΕΙΩΝ': 'K',
                  'ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)': 'L'},
          inplace=True)

df.columns

# Create a simple heatmap to see if we got missing data!
sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')

"""So, we dont have any missing data in our dataset."""

# Difference in number of data in class 1 and 2
sns.countplot(data=df,x = 'L')

# Count the values of column L, for each value
df['L'].value_counts()

"""As we can see, there is a big difference between two classes, so it can be challenging to achieve high accuracy for both classes simultaneously.
- 'L' is is the value we want to predict

- Because the difference of the values in column L is huge, it is obvious that we cannot make a correct predictions for businesses, whether they will go bankrupt or not. And so in the initial tests the results were 98% for 1 and 2% for 2. So I tried the smote analysis where it gave me much better results.
(Synthetic Minority Oversampling Technique (SMOTE) is a statistical technique for increasing the number of cases in your dataset in a balanced way. The component works by generating new instances from existing minority cases that you supply as input.)
"""

# Separate the target variable
X = df.drop('L', axis=1)
y = df['L']

# Perform train-test split on the standardized data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE for oversampling
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Normalize the resampled data
scaler = StandardScaler()
X_resampled = scaler.fit_transform(X_resampled)
X_test_scaled = scaler.transform(X_test)

# Number of Nonhealthy Companies in training sample
y_resampled[y_resampled==2].count()

"""# 1) Linear Discriminant Analysis"""

# Apply Linear Discriminant Analysis on the resampled data
lda = LinearDiscriminantAnalysis()
lda.fit(X_resampled, y_resampled)

# Make predictions on the scaled training and test data
y_train_pred = lda.predict(X_resampled)
y_test_pred = lda.predict(X_test_scaled)

# Separate predictions for value 1 and value 2
pred_value_1 = X_test[y_test_pred == 1]
pred_value_2 = X_test[y_test_pred == 2]

# Accuracy for 1 (All good)
accuracy_1 = accuracy_score(y_test[y_test == 1], y_test_pred[y_test == 1])
accuracy_1

# Accuracy for 2 (Has declared bankruptcy)
accuracy_2 = accuracy_score(y_test[y_test == 2], y_test_pred[y_test == 2])
accuracy_2

# Calculate evaluation metrics for the training set
accuracy_train = accuracy_score(y_resampled, y_train_pred)
precision_train = precision_score(y_resampled, y_train_pred)
recall_train = recall_score(y_resampled, y_train_pred)
f1_train = f1_score(y_resampled, y_train_pred)

cm_train = confusion_matrix(y_resampled, y_train_pred)
print('Confussion matrix for training set: \n',cm_train)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train)
print('Precision: ',precision_train)
print('Recall: ',recall_train)
print('F1 score: ', f1_train)

# Calculate evaluation metrics for the test set
accuracy_test = accuracy_score(y_test, y_test_pred)
precision_test = precision_score(y_test, y_test_pred)
recall_test = recall_score(y_test, y_test_pred)
f1_test = f1_score(y_test, y_test_pred)

cm_test = confusion_matrix(y_test, y_test_pred)
print('Confussion matrix for test set: \n', cm_test)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test)
print('Precision: ',precision_test)
print('Recall: ',recall_test)
print('F1 score: ', f1_test)

"""# 2) Logistic Regression"""

# Apply Logistic Regression and fit the model
log = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', class_weight={1: 0.95, 2: 1.4})
log.fit(X_resampled, y_resampled)

# Make predictions on the scaled training and test data
predictions_y_train = log.predict(X_resampled)
predictions_y_test  = log.predict(X_test_scaled)

# Separate predictions for value 1 and value 2 (Logistic regression model)
pred_log_1 = X_test[predictions_y_test  == 1]
pred_log_2 = X_test[predictions_y_test  == 2]

# Accuracy for 1 (All good)
accuracy_log_1 = accuracy_score(y_test[y_test == 1], predictions_y_test[y_test == 1])
accuracy_log_1

#Accuracy for 2 (Has declared bankruptcy)
accuracy_log_2 = accuracy_score(y_test[y_test == 2], predictions_y_test[y_test == 2])
accuracy_log_2

# Calculate evaluation metrics for the training set
accuracy_train_log = accuracy_score(y_resampled, predictions_y_train)
precision_train_log = precision_score(y_resampled, predictions_y_train)
recall_train_log = recall_score(y_resampled, predictions_y_train)
f1_train_log = f1_score(y_resampled, predictions_y_train)

cm_train_log = confusion_matrix(y_resampled, predictions_y_train)
print('Confussion matrix for training set: \n', cm_train_log)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train_log)
print('Precision: ',precision_train_log)
print('Recall: ',recall_train_log)
print('F1 score: ', f1_train_log)

# Calculate evaluation metrics for the test set
accuracy_test_log = accuracy_score(y_test, predictions_y_test)
precision_test_log = precision_score(y_test, predictions_y_test)
recall_test_log = recall_score(y_test, predictions_y_test)
f1_test_log = f1_score(y_test, predictions_y_test)

cm_test_log = confusion_matrix(y_test, predictions_y_test)
print('Confussion matrix for test set: \n', cm_test_log)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test_log)
print('Precision: ',precision_test_log)
print('Recall: ',recall_test_log)
print('F1 score: ', f1_test_log)

"""# 3) Decision Trees"""

np.random.seed(42)
# Apply Decision Trees and fit the model
clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5,
                              max_features="sqrt", class_weight={1: 0.7, 2 : 1.2})
# I did various experiments with the parameters,
# so I tweaked the parameters so I could get better results to hit the hit rate constraints

# Train the classifier
clf.fit(X_resampled, y_resampled)

# Make predictions on the scaled training and test data
predictions_y_train_tr = clf.predict(X_resampled)
predictions_y_test_tr  = clf.predict(X_test_scaled)

# Separate predictions for value 1 and value 2 (Decision Trees model)
pred_log_1 = X_test[predictions_y_test_tr  == 1]
pred_log_2 = X_test[predictions_y_test_tr  == 2]

# Accuracy for 1 (All good)
accuracy_tr_1 = accuracy_score(y_test[y_test == 1], predictions_y_test_tr[y_test == 1])
accuracy_tr_1

#Accuracy for 2 (Has declared bankruptcy)
accuracy_tr_2 = accuracy_score(y_test[y_test == 2], predictions_y_test_tr[y_test == 2])
accuracy_tr_2

# Calculate evaluation metrics for the training set
accuracy_train_tr = accuracy_score(y_resampled, predictions_y_train_tr)
precision_train_tr = precision_score(y_resampled, predictions_y_train_tr)
recall_train_tr = recall_score(y_resampled, predictions_y_train_tr)
f1_train_tr = f1_score(y_resampled, predictions_y_train_tr)

cm_train_tr = confusion_matrix(y_resampled, predictions_y_train_tr)
print('Confussion matrix for training set: \n',cm_train_tr)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train_tr)
print('Precision: ',precision_train_tr)
print('Recall: ',recall_train_tr)
print('F1 score: ', f1_train_tr)

# Calculate evaluation metrics for the test set
accuracy_test_tr = accuracy_score(y_test, predictions_y_test_tr)
precision_test_tr = precision_score(y_test, predictions_y_test_tr)
recall_test_tr = recall_score(y_test, predictions_y_test_tr)
f1_test_tr = f1_score(y_test, predictions_y_test_tr)

cm_test_tr = confusion_matrix(y_test, predictions_y_test_tr)
print('Confussion matrix for test set: \n',cm_test_tr)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test_tr)
print('Precision: ',precision_test_tr)
print('Recall: ',recall_test_tr)
print('F1 score: ', f1_test_tr)

"""# 4) k-Nearest Neighbors"""

# Apply kNN and fit the model
knn = KNeighborsClassifier(n_neighbors=100, leaf_size=30, p=1)
knn.fit(X_resampled, y_resampled)

# I did several empirical experiments changing the n_neighbors. After 100, nothing changes.
# ( Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance)

# Make predictions on the scaled training and test data
predictions_y_train_knn = knn.predict(X_resampled)
predictions_y_test_knn  = knn.predict(X_test_scaled)

# Separate predictions for value 1 and value 2 (k-NN model)
pred_knn_1 = X_test[predictions_y_test_knn  == 1]
pred_knn_2 = X_test[predictions_y_test_knn  == 2]

# Accuracy for 1 (All good)
accuracy_knn_1 = accuracy_score(y_test[y_test == 1], predictions_y_test_knn[y_test == 1])
accuracy_knn_1

#Accuracy for 2 (Has declared bankruptcy)
accuracy_knn_2 = accuracy_score(y_test[y_test == 2], predictions_y_test_knn[y_test == 2])
accuracy_knn_2

# Calculate evaluation metrics for the training set
accuracy_train_knn = accuracy_score(y_resampled, predictions_y_train_knn)
precision_train_knn = precision_score(y_resampled, predictions_y_train_knn)
recall_train_knn = recall_score(y_resampled, predictions_y_train_knn)
f1_train_knn = f1_score(y_resampled, predictions_y_train_knn)

cm_train_knn = confusion_matrix(y_resampled, predictions_y_train_knn)
print('Confussion matrix for training set: \n',cm_train_knn)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train_knn)
print('Precision: ',precision_train_knn)
print('Recall: ',recall_train_knn)
print('F1 score: ', f1_train_knn)

# Calculate evaluation metrics for the test set
accuracy_test_knn = accuracy_score(y_test, predictions_y_test_knn)
precision_test_knn = precision_score(y_test, predictions_y_test_knn)
recall_test_knn = recall_score(y_test, predictions_y_test_knn)
f1_test_knn = f1_score(y_test, predictions_y_test_knn)

cm_test_knn = confusion_matrix(y_test, predictions_y_test_knn)
print('Confussion matrix for test set: \n',cm_test_knn)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test_knn)
print('Precision: ',precision_test_knn)
print('Recall: ',recall_test_knn)
print('F1 score: ', f1_test_knn)

"""# 5) Naïve Bayes"""

# Apply Bernoulli Naïve Bayes and fit the model
nb = BernoulliNB()
nb.fit(X_resampled, y_resampled)

# I used Naive Bayes but no matter what test I did, I didn't get the results I needed (1>70% and 2>62%).
# So i tried 'BernoulliNB' that is a suitable variation of the Naïve Bayes algorithm for binary feature data.
# It assumes that the features follow a Bernoulli distribution, meaning they are binary variables (0 or 1).

# Make predictions on the scaled training and test data
predictions_y_train_nb = nb.predict(X_resampled)
predictions_y_test_nb  = nb.predict(X_test_scaled)

# Separate predictions for value 1 and value 2 (Naïve Bayes model)
pred_nb_1 = X_test[predictions_y_test_nb  == 1]
pred_nb_2 = X_test[predictions_y_test_nb  == 2]

# Accuracy for 1 (All good)
accuracy_nb_1 = accuracy_score(y_test[y_test == 1], predictions_y_test_nb[y_test == 1])
accuracy_nb_1

#Accuracy for 2 (Has declared bankruptcy)
accuracy_nb_2 = accuracy_score(y_test[y_test == 2], predictions_y_test_nb[y_test == 2])
accuracy_nb_2

# Calculate evaluation metrics for the training set
accuracy_train_nb = accuracy_score(y_resampled, predictions_y_train_nb)
precision_train_nb = precision_score(y_resampled, predictions_y_train_nb)
recall_train_nb = recall_score(y_resampled, predictions_y_train_nb)
f1_train_nb = f1_score(y_resampled, predictions_y_train_nb)

cm_train_nb = confusion_matrix(y_resampled, predictions_y_train_nb)
print('Confussion matrix for training set: \n',cm_train_nb)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train_nb)
print('Precision: ',precision_train_nb)
print('Recall: ',recall_train_nb)
print('F1 score: ', f1_train_nb)

# Calculate evaluation metrics for the test set
accuracy_test_nb = accuracy_score(y_test, predictions_y_test_nb)
precision_test_nb = precision_score(y_test, predictions_y_test_nb)
recall_test_nb = recall_score(y_test, predictions_y_test_nb)
f1_test_nb = f1_score(y_test, predictions_y_test_nb)

cm_test_nb = confusion_matrix(y_test, predictions_y_test_nb)
print('Confussion matrix for test set: \n',cm_test_nb)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test_nb)
print('Precision: ',precision_test_nb)
print('Recall: ',recall_test_nb)
print('F1 score: ', f1_test_nb)

"""# 6) Support Vector Machines"""

# Apply SVC and fit the model
svm = SVC(C=0.1,kernel='linear')
svm.fit(X_resampled, y_resampled)

# The C parameter in SVM determines the trade-off between maximizing the margin and minimizing the classification errors,
# and the best value was 0.01 after experiments.
# SVM algorithms utilize different kernel functions to map the input data into a higher-dimensional feature space.
# The choice of kernel can have a significant impact on the SVM's performance.
# By default, SVC in scikit-learn uses the Radial Basis Function (RBF) kernel, but 'Linear'
# got better capture in the underlying patterns in our data.

# Make predictions on the scaled training and test data
predictions_y_train_svm = svm.predict(X_resampled)
predictions_y_test_svm  = svm.predict(X_test_scaled)

# Separate predictions for value 1 and value 2 (SVM model)
pred_svm_1 = X_test[predictions_y_test_svm   == 1]
pred_svm_2 = X_test[predictions_y_test_svm   == 2]

# Accuracy for 1 (All good)
accuracy_svm_1 = accuracy_score(y_test[y_test == 1], predictions_y_test_svm[y_test == 1])
accuracy_svm_1

#Accuracy for 2 (Has declared bankruptcy)
accuracy_svm_2 = accuracy_score(y_test[y_test == 2], predictions_y_test_svm[y_test == 2])
accuracy_svm_2

# Calculate evaluation metrics for the training set
accuracy_train_svm = accuracy_score(y_resampled, predictions_y_train_svm)
precision_train_svm = precision_score(y_resampled, predictions_y_train_svm)
recall_train_svm = recall_score(y_resampled, predictions_y_train_svm)
f1_train_svm = f1_score(y_resampled, predictions_y_train_svm)

cm_train_svm = confusion_matrix(y_resampled, predictions_y_train_svm)
print('Confussion matrix for training set: \n',cm_train_svm)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train_svm)
print('Precision: ',precision_train_svm)
print('Recall: ',recall_train_svm)
print('F1 score: ', f1_train_svm)

# Calculate evaluation metrics for the test set
accuracy_test_svm = accuracy_score(y_test, predictions_y_test_svm)
precision_test_svm = precision_score(y_test, predictions_y_test_svm)
recall_test_svm = recall_score(y_test, predictions_y_test_svm)
f1_test_svm = f1_score(y_test, predictions_y_test_svm)

cm_test_svm = confusion_matrix(y_test, predictions_y_test_svm)
print('Confussion matrix for test set: \n',cm_test_svm)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test_svm)
print('Precision: ',precision_test_svm)
print('Recall: ',recall_test_svm)
print('F1 score: ', f1_test_svm)

"""# 7) Neural Networks"""

# Split the data into training, validation, and test sets
X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)

# Apply SMOTE to the training set
smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Train the neural network model
model = MLPClassifier(hidden_layer_sizes=(2,5), activation='logistic', solver='sgd',
                      batch_size=50,learning_rate_init=0.001,max_iter=500, random_state=42)

model.fit(X_train_scaled, y_train_smote)

# I did a lot of testing to get the right parameters for the highest accuracy rate for each class.

# Predict on the training set
y_train_pred = model.predict(X_train_scaled)
train_accuracy = accuracy_score(y_train_smote, y_train_pred)

# Predict on the validation set
y_val_pred = model.predict(X_val_scaled)
val_accuracy = accuracy_score(y_val, y_val_pred)

# Predict on the test set
y_test_pred = model.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)

# Print the accuracy for each set
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)
print("Test Accuracy:", test_accuracy)

# Separate predictions for value 1 and value 2 (MLP model)
pred_nn_1 = X_train_scaled[y_train_pred   == 1]
pred_nn_2 = X_train_scaled[y_train_pred   == 2]

# Calculate the accuracy for Class 1
class_1_accuracy = accuracy_score(y_test[y_test == 1], y_test_pred[y_test == 1])

# Print the accuracy for Class 1
print("Accuracy for Class 1:", class_1_accuracy)

# Calculate the accuracy for Class 2
class_2_accuracy = accuracy_score(y_test[y_test == 2], y_test_pred[y_test == 2])

# Print the accuracy for Class 2
print("Accuracy for Class 2:", class_2_accuracy)

# Calculate evaluation metrics for the training set
accuracy_train_nn = accuracy_score(y_resampled, y_train_pred)
precision_train_nn = precision_score(y_resampled, y_train_pred)
recall_train_nn = recall_score(y_resampled, y_train_pred)
f1_train_nn = f1_score(y_resampled, y_train_pred)

cm_train_nn = confusion_matrix(y_resampled, y_train_pred)
print('Confussion matrix for training set: \n',cm_train_nn)
print('\nFor training set:')
print('\nAccuracy: ', accuracy_train_nn)
print('Precision: ',precision_train_nn)
print('Recall: ',recall_train_nn)
print('F1 score: ', f1_train_nn)

# Calculate evaluation metrics for the test set
accuracy_test_nn = accuracy_score(y_test, y_test_pred)
precision_test_nn = precision_score(y_test, y_test_pred)
recall_test_nn = recall_score(y_test, y_test_pred)
f1_test_nn = f1_score(y_test, y_test_pred)

cm_test_nn = confusion_matrix(y_test, y_test_pred)
print('Confussion matrix for test set: \n',cm_test_nn)
print('\nFor test set:')
print('\nAccuracy: ', accuracy_test_nn)
print('Precision: ',precision_test_nn)
print('Recall: ',recall_test_nn)
print('F1 score: ', f1_test_nn)





